{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# QA\n",
    "inputs = [\n",
    "    \"LangSmithの3つの主な機能は？\",\n",
    "    \"Tracingはどういう単位で記録されますか？\",\n",
    "    \"PharmaXではもともとどのような評価ツールを使っていましたか？\"\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    \"TracingとEvaluationとPromptsです。\",\n",
    "    \"Runという単位で記録されます。\",\n",
    "    \"PromptLayerです。\"\n",
    "]\n",
    "\n",
    "qa_pairs = [{\"question\": q, \"answer\": a} for q, a in zip(inputs, outputs)]\n",
    "df = pd.DataFrame(qa_pairs)\n",
    "\n",
    "csv_path = \"./LangSmith.csv\"\n",
    "df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"LangSmith\"\n",
    "\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"QA pairs about LangSmith Blog Article.\",\n",
    ")\n",
    "\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in inputs],\n",
    "    outputs=[{\"answer\": a} for a in outputs],\n",
    "    dataset_id=dataset.id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_questions = [\n",
    "    \"LangSmithでトレースを開始するためにはどうすればいいですか？\"\n",
    "]\n",
    "\n",
    "new_answers = [\n",
    "    \"環境変数を設定します。\"\n",
    "]\n",
    "\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in new_questions],\n",
    "    outputs=[{\"answer\": a} for a in new_answers],\n",
    "    dataset_id=dataset.id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://zenn.dev/pharmax/articles/61edc477e4de17\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "text = [p.text for p in soup.find_all(\"p\")]\n",
    "full_text = \"\\n\".join(text)\n",
    "print(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from langsmith.wrappers import wrap_openai\n",
    "\n",
    "openai_client = wrap_openai(openai.Client())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_langsmith_question_oai(inputs: dict) -> dict:\n",
    "    system_msg = f\"以下のコンテキストをもとに、ユーザの質問に2～3文程度で回答してください。\\n\\n\\n{full_text}\"\n",
    "\n",
    "    messages = [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_msg\n",
    "    }, {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": inputs[\"question\"]\n",
    "    }]\n",
    "\n",
    "    response = openai_client.chat.completions.create(messages=messages, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "    return {\"answer\": response.choices[0].message.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_langsmith_question_oai({\"question\": \"LangSmithの3つの主な機能は？\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_langsmith_question_oai({\"question\": \"LangSmithでトレースを開始するためにはどうすればいいですか？\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "qa_evaluator = [LangChainStringEvaluator(\"cot_qa\")]\n",
    "dataset_name = \"LangSmith\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = evaluate(\n",
    "    answer_langsmith_question_oai,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evaluator,\n",
    "    experiment_prefix=\"test-langsmith-qa-gpt-3.5-turbo\",\n",
    "    metadata={\n",
    "        \"variant\": \"LangSmithのブログ記事をコンテキストにしたQA、gpt-3.5-turboを利用\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "qa_evaluator = [LangChainStringEvaluator(\"qa\")]\n",
    "dataset_name = \"LangSmith\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = evaluate(\n",
    "    answer_langsmith_question_oai,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evaluator,\n",
    "    experiment_prefix=\"test-langsmith-qa-oai\",\n",
    "    metadata={\n",
    "        \"variant\": \"LangSmithのブログ記事をコンテキストにしたQA、gpt-4o-miniを利用\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## カスタム評価関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.schemas import Run, Example\n",
    "\n",
    "\n",
    "# ルールベースの評価関数\n",
    "# run: LLMの出力\n",
    "# example: Ground TruthのQAペア\n",
    "def is_answered(run: Run, example: Example) -> dict:\n",
    "    # LLMの出力のこと\n",
    "    student_answer = run.outputs.get(\"answer\")\n",
    "\n",
    "    # answerが含まれているかチェック\n",
    "    if not student_answer:\n",
    "        return {\"key\": \"is_answered\", \"score\": 0}\n",
    "    else:\n",
    "        return {\"key\": \"is_answered\", \"score\": 1}\n",
    "\n",
    "\n",
    "qa_evaluator = [is_answered]\n",
    "dataset_name = \"LangSmith\"\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    answer_langsmith_question_oai,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evaluator,\n",
    "    experiment_prefix=\"test-langsmith-qa-custom-eval-is-answered\",\n",
    "    metadata={\n",
    "        \"variant\": \"LangSmithのブログ記事をコンテキストにしたQA、gpt-4o-miniを利用\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "examples = [\n",
    "    (\"Ankush\", \"Hello Ankush\"),\n",
    "    (\"Harrison\", \"Hello Harrison\"),\n",
    "]\n",
    "\n",
    "dataset_name = \"Hello Set\"\n",
    "dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "inputs, outputs = zip(\n",
    "    *[({\"input\": input}, {\"expected\": expected}) for input, expected in examples]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_examples(\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    dataset_id=dataset.id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"Hello Set wo Reference\"\n",
    "dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "inputs = [{\"input\": input} for input, _ in examples]\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_examples(\n",
    "    inputs=inputs,\n",
    "    dataset_id=dataset.id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "\n",
    "cot_qa_evaluator = LangChainStringEvaluator(\"cot_qa\")\n",
    "\n",
    "evaluate(\n",
    "    lambda input: \"Hello \" + input[\"input\"],\n",
    "    data=dataset_name,\n",
    "    evaluators=[cot_qa_evaluator],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "\n",
    "criteria_evaluator = LangChainStringEvaluator(\n",
    "    \"criteria\",\n",
    "    config={\n",
    "        \"criteria\": {\n",
    "            \"say_hello\": \"Does the submission say hello?\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "evaluate(\n",
    "    lambda input: \"Hello \" + input[\"input\"],\n",
    "    # GTがないデータセットに対しても適用できる\n",
    "    data=\"Hello Set wo Reference\",\n",
    "    evaluators=[criteria_evaluator],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "\n",
    "criteria_evaluator = LangChainStringEvaluator(\n",
    "    \"labeled_criteria\",\n",
    "    config={\n",
    "        \"criteria\": {\n",
    "            \"creativity\": \"Is this submission creative, imaginative, or novel?\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "evaluate(\n",
    "    lambda input: \"Hello \" + input[\"input\"],\n",
    "    data=\"Hello Set\",\n",
    "    evaluators=[criteria_evaluator],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAGの検索の評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "inputs = [\n",
    "    {\"question\": \"agent memory\", \"doc_txt\": \"agent memory has two types: short and long term\"},\n",
    "    {\"question\": \"hallucinations\", \"doc_txt\": \"DBRX was pretrained on 12T tokens\"},\n",
    "    {\"question\": \"DBRX content window\", \"doc_txt\": \"DBRX has a 32K token context window\"},\n",
    "]\n",
    "\n",
    "# reference\n",
    "# yes => relevant\n",
    "# no => not relevant\n",
    "outputs = [\n",
    "    \"yes\",\n",
    "    \"no\",\n",
    "    \"yes\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"Relevance_grade\"\n",
    "\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Testing relevance grading.\"\n",
    ")\n",
    "\n",
    "client.create_examples(\n",
    "    inputs=inputs,\n",
    "    outputs=[{\"answer\": a} for a in outputs],\n",
    "    dataset_id=dataset.id\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning-langchain-BRoTc1ZH-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
