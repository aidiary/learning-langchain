{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://python.langchain.com/docs/expression_language/\"\n",
    "loader = RecursiveUrlLoader(url=url, max_depth=20, extractor=lambda x: BeautifulSoup(x, \"html.parser\").text)\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"What is LCEL?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG\n",
    "import openai\n",
    "from langsmith import traceable\n",
    "from langsmith.wrappers import wrap_openai\n",
    "\n",
    "\n",
    "class RagBot:\n",
    "    def __init__(self, retriever, model: str=\"gpt-4o-mini\"):\n",
    "        self._retriever = retriever\n",
    "        self._client = wrap_openai(openai.Client())\n",
    "        self._model = model\n",
    "\n",
    "    @traceable\n",
    "    def get_answer(self, question: str):\n",
    "        similar = self._retriever.invoke(question)\n",
    "        response = self._client.chat.completions.create(\n",
    "            model=self._model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a helpful AI assistant with expertise in LCEL.\"\n",
    "                    \" Use the following docs to produce a concise code solution to the user question.\\n\\n\"\n",
    "                    f\"## Docs\\n\\n{similar}\",\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"answer\": response.choices[0].message.content,\n",
    "            \"contexts\": [str(doc) for doc in similar],\n",
    "        }\n",
    "\n",
    "rag_bot = RagBot(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_bot.get_answer(\"What is LCEL?\")\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG dataset\n",
    "from langsmith import Client\n",
    "\n",
    "inputs = [\n",
    "    \"How can I directly pass a string to a runnable and use it to construct the input needed for my prompt?\",\n",
    "    \"How can I make the output of my LCEL chain a string?\",\n",
    "    \"How can I apply a custom function to one of the inputs of an LCEL chain?\"\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    \"Use RunnablePassthrough.\",\n",
    "    \"Use StrOutputParser.\",\n",
    "    \"Use RunnableLambda with itemgetter to extract the relevant key.\"\n",
    "]\n",
    "\n",
    "qa_pairs = [{\"question\": q, \"answer\": a} for q, a in zip(inputs, outputs)]\n",
    "qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "dataset_name = \"RAG_test_LCEL\"\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"QA pairs about LCEL.\"\n",
    ")\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in inputs],\n",
    "    outputs=[{\"answer\": a} for a in outputs],\n",
    "    dataset_id=dataset.id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG chain\n",
    "def predict_rag_answer(example: dict):\n",
    "    response = rag_bot.get_answer(example[\"question\"])\n",
    "    return {\"answer\": response[\"answer\"]}\n",
    "\n",
    "\n",
    "def predict_rag_answer_with_context(example: dict):\n",
    "    response = rag_bot.get_answer(example[\"question\"])\n",
    "    return {\"answer\": response[\"answer\"], \"contexts\": response[\"contexts\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "\n",
    "# 予測と正解の比較\n",
    "# Answer Accuracy\n",
    "\"\"\"\n",
    "あなたは小テストを採点する教師です。\n",
    "あなたには問題、問題の背景、学生の答えが与えられます。あなたは文脈に基づいて、生徒の答えを「正解」または「不正解」のどちらかに採点するよう求められます。\n",
    "あなたの結論が正しいことを確認するために、あなたの推論を段階的に書き出してください。最初に単に正解を述べることは避けてください。\n",
    "\n",
    "フォーマット例\n",
    "QUESTION: ここに質問\n",
    "CONTEXT：質問の文脈はここ\n",
    "STUDENT ANSWER：学生の答えはここ\n",
    "EXPLANATION: ここでのステップごとの推論\n",
    "GRADE: CORRECT（正解）またはINCORRECT（不正解）をここに記入します。\n",
    "\n",
    "生徒の解答は事実の正確さのみに基づいて採点します。学生の解答と本当の解答の句読点や言い回しの違いは無視します。矛盾する記述がない限り、生徒の答えが本当の答えより多くの情報を含んでいても構いません。開始します！\n",
    "\"\"\"\n",
    "\n",
    "qa_evaluator = [\n",
    "    LangChainStringEvaluator(\"cot_qa\",\n",
    "                             # これは、あってもなくても同じではないか？\n",
    "                             prepare_data=lambda run, example: {\n",
    "                                 \"prediction\": run.outputs[\"answer\"],\n",
    "                                 \"reference\": example.outputs[\"answer\"],\n",
    "                                 \"input\": example.inputs[\"question\"],\n",
    "                             })]\n",
    "\n",
    "dataset_name = \"RAG_test_LCEL\"\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evaluator,\n",
    "    experiment_prefix=\"rag-qa-oai\",\n",
    "    metadata={\"variant\": \"LCEL context, gpt-3.5-turbo\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hallucinationの評価\n",
    "# 検索した文書から生成した回答が得られるか？\n",
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "\n",
    "answer_hallucination_evaluator = LangChainStringEvaluator(\n",
    "    \"labeled_score_string\",\n",
    "    config={\n",
    "        \"criteria\": {\n",
    "            \"accuracy\": \"\"\"Is the Assistant's Answer grounded in the Ground Truth documentation? A score of 0 means that the\n",
    "            Assistant answer contains is not at all based upon / grounded in the Ground Truth documentation. A score of 5 means\n",
    "            that the Assistant answer contains some information (e.g., a hallucination) that is not captured in the Ground Truth\n",
    "            documentation. A score of 10 means that the Assistant answer is fully based upon the in the Ground Truth documentation.\"\"\"\n",
    "        },\n",
    "        \"normalized_by\": 10\n",
    "    },\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"answer\"],\n",
    "        \"reference\": run.outputs[\"contexts\"],\n",
    "        \"input\": example.inputs[\"question\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"RAG_test_LCEL\"\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_with_context,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_hallucination_evaluator],\n",
    "    experiment_prefix=\"rag-qa-oai-hallucination\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning-langchain-BRoTc1ZH-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
