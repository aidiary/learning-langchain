{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Iteration Walkthrough\n",
    "\n",
    "https://github.com/langchain-ai/langsmith-cookbook/blob/main/testing-examples/movie-demo/prompt_iteration.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_examples = [\n",
    "    (\"Shut up, idiot\", \"Toxic\"),\n",
    "    (\"You're a wonderful person\", \"Not toxic\"),\n",
    "    (\"This is the worst thing ever\", \"Toxic\"),\n",
    "    (\"I had a great day today\", \"Not toxic\"),\n",
    "    (\"Nobody likes you\", \"Toxic\"),\n",
    "    (\"This movie is a masterpiece\", \"Not toxic\"),\n",
    "    (\"Go away and never come back\", \"Toxic\"),\n",
    "    (\"Thank you for your help\", \"Not toxic\"),\n",
    "    (\"This is so dumb\", \"Toxic\"),\n",
    "    (\"I appreciate your efforts\", \"Not toxic\"),\n",
    "    (\"This is a waste of time\", \"Toxic\"),\n",
    "    (\"This movie blows\", \"Toxic\"),\n",
    "    (\"This is unacceptable. I want to speak to the manager.\", \"Toxic\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_dataset_name = \"Toxic Queires\"\n",
    "\n",
    "if not client.has_dataset(dataset_name=toxic_dataset_name):\n",
    "    toxic_dataset = client.create_dataset(dataset_name=toxic_dataset_name)\n",
    "    inputs, outputs = zip(\n",
    "        *[({\"text\": text}, {\"label\": label}) for text, label in toxic_examples]\n",
    "    )\n",
    "    client.create_examples(inputs=inputs, outputs=outputs, dataset_id=toxic_dataset.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable, wrappers\n",
    "from openai import Client\n",
    "\n",
    "openai = wrappers.wrap_openai(Client())\n",
    "\n",
    "@traceable\n",
    "def label_query(text):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Given the following user query, assess whether it contains toxic content. Please provide a sample 'Tocic' or 'Not toxic' response based on your assessment.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    result = openai.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0\n",
    "    )\n",
    "    return result.choices[0].message.content\n",
    "\n",
    "\n",
    "@traceable\n",
    "def label_query_alternate_prompt(text):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    result = openai.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0\n",
    "    )\n",
    "    return result.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_query(\"You're a wonderful person\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "\n",
    "# row-level evaluator\n",
    "def correct_label(run, example) -> dict:\n",
    "    score = run.outputs.get(\"output\") == example.outputs.get(\"label\")\n",
    "    return {\"score\": int(score)}\n",
    "\n",
    "\n",
    "# summary (experiment-level) evaluator\n",
    "def summary_eval(runs, examples):\n",
    "    correct = 0\n",
    "    for i, run in enumerate(runs):\n",
    "        if run.outputs[\"output\"] == examples[i].outputs[\"label\"]:\n",
    "            correct += 1\n",
    "    if correct / len(runs) > 0.5:\n",
    "        return {\"key\": \"pass\", \"score\": True}\n",
    "    else:\n",
    "        return {\"key\": \"pass\", \"score\": False}\n",
    "\n",
    "\n",
    "result_1 = evaluate(\n",
    "    lambda inputs: label_query(inputs[\"text\"]),\n",
    "    data=toxic_dataset_name,\n",
    "    evaluators=[correct_label],\n",
    "    summary_evaluators=[summary_eval],\n",
    "    experiment_prefix=\"Toxic Queries\",\n",
    "    metadata={\n",
    "        \"prompt_version\": \"1\"\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2 = evaluate(\n",
    "    lambda inputs: label_query_alternate_prompt(inputs[\"text\"]),\n",
    "    data=toxic_dataset_name,\n",
    "    evaluators=[correct_label],\n",
    "    summary_evaluators=[summary_eval],\n",
    "    experiment_prefix=\"Toxic Queries\",\n",
    "    metadata={\n",
    "        \"prompt_version\": \"2\"\n",
    "    },\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning-langchain-BRoTc1ZH-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
