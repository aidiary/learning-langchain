{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangSmith Evaluation Deep Dive\n",
    "\n",
    "https://github.com/langchain-ai/langsmith-cookbook/blob/main/introduction/langsmith_introduction.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手動によるデータセットの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# QA\n",
    "inputs = [\n",
    "    \"How many tokens was DBRX pre-trained on?\",\n",
    "    \"Is DBRX a MOE model and how many parameters does it have?\",\n",
    "    \"How many GPUs was DBRX trained on and what was the connectivity between GPUs?\",\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    \"DBRX was pre-trained on 12 trillion tokens of text and code data.\",\n",
    "    \"Yes, DBRX is a fine-grained mixture-of-experts (MoE) architecture with 132B total parameters.\",\n",
    "    \"DBRX was trained on 3072 NVIDIA H100s connected by 3.2Tbps Infiniband\",\n",
    "]\n",
    "\n",
    "qa_pairs = [{\"questions\": q, \"answer\": a} for q, a in zip(inputs, outputs)]\n",
    "df = pd.DataFrame(qa_pairs)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"./DBRX_eval.csv\"\n",
    "df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"DBRX\"\n",
    "\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"QA pairs about DBRX model.\",\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in inputs],\n",
    "    outputs=[{\"answer\": a} for a in outputs],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_questions = [\n",
    "    \"What is the context window of DBRX Instruct?\",\n",
    "]\n",
    "\n",
    "new_answers = [\n",
    "    \"DBRX Instruct was trained with up to a 32K token context window.\",\n",
    "]\n",
    "\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in new_questions],\n",
    "    outputs=[{\"answer\": a} for a in new_answers],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 質問に対する回答を生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"DBRX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "text = [p.text for p in soup.find_all(\"p\")]\n",
    "full_text = \"\\n\".join(text)\n",
    "\n",
    "print(full_text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from langsmith.wrappers import wrap_openai\n",
    "\n",
    "openai_client = wrap_openai(openai.Client())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_dbrx_question_oai(inputs: dict) -> dict:\n",
    "    system_msg = (\n",
    "        f\"Answer user questions in 2-3 sentences about this context: \\n\\n\\n {full_text}\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\"role\": \"user\", \"content\": inputs[\"question\"]}\n",
    "    ]\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=\"gpt-4o-mini\",\n",
    "    )\n",
    "\n",
    "    return {\"answer\": response.dict()[\"choices\"][0][\"message\"][\"content\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_dbrx_question_oai({\"question\": \"What are the main differences in training efficiency between MPT-7B vs DBRX?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_dbrx_question_oai({\"question\": \"How many tokens was DBRX pre-trained on?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM-as-a-Judgeによる評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "qa_evaluator = [LangChainStringEvaluator(\"cot_qa\")]\n",
    "dataset_name = \"DBRX\"\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    answer_dbrx_question_oai,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evaluator,\n",
    "    experiment_prefix=\"test-dbrx-qa-oai\",\n",
    "    metadata={\n",
    "        \"variant\": \"stuff website context into gpt-4o-mini\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.schemas import Run, Example\n",
    "\n",
    "\n",
    "def is_answered(run: Run, example: Example) -> dict:\n",
    "    student_answer = run.outputs.get(\"answer\")\n",
    "\n",
    "    if not student_answer:\n",
    "        return {\"key\": \"is_answered\", \"score\": 0}\n",
    "    else:\n",
    "        return {\"key\": \"is_answered\", \"score\": 1}\n",
    "\n",
    "\n",
    "qa_evaluator = [is_answered]\n",
    "dataset_name = \"DBRX\"\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    answer_dbrx_question_oai,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evaluator,\n",
    "    experiment_prefix=\"test-dbrx-qa-custom-eval-is-answered\",\n",
    "    metadata={\n",
    "        \"variant\": \"stuff website context into gpt-4o-mini\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistralとの比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from langsmith.run_helpers import traceable\n",
    "\n",
    "\n",
    "@traceable(run_type=\"llm\")\n",
    "def call_ollama(messages, model: str):\n",
    "    stream = ollama.chat(messages=messages, model=model, stream=True)\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "        response = response + chunk[\"message\"][\"content\"]\n",
    "    return response\n",
    "\n",
    "\n",
    "def answer_dbrx_question_mistral(inputs: dict) -> dict:\n",
    "    system_msg = (\n",
    "        f\"Answer user questions about this context: \\n\\n\\n {full_text}\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\"role\": \"user\", \"content\": f'Answer the question in 2-3 sentences {inputs[\"question\"]}'}\n",
    "    ]\n",
    "\n",
    "    response = call_ollama(messages, model=\"mistral\")\n",
    "\n",
    "    return {\"answer\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = answer_dbrx_question_mistral({\"question\": \"What are the main differences in training efficiency between MPT-7B vs DBRX?\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mistral Evaluators\n",
    "qa_evaluator = [LangChainStringEvaluator(\"cot_qa\")]\n",
    "dataset_name = \"DBRX\"\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    answer_dbrx_question_mistral,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evaluator,\n",
    "    experiment_prefix=\"test-dbrx-qa-mistral\",\n",
    "    metadata={\n",
    "        \"variant\": \"stuff website context into mistral\",\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning-langchain-BRoTc1ZH-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
