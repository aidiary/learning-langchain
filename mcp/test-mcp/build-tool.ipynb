{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ede2670",
   "metadata": {},
   "source": [
    "# MCP From Scratch\n",
    "\n",
    "- LangGraph のドキュメントをもとに回答する RAG を MCP ツールとして登録する\n",
    "- https://mirror-feeling-d80.notion.site/MCP-From-Scratch-1b9808527b178040b5baf83a991ed3b2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0faf343c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9f4f0a",
   "metadata": {},
   "source": [
    "## RAG を行う LangChain のツールを作成\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5414780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import tiktoken\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import SKLearnVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2767383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text, model=\"cl100k_base\"):\n",
    "    \"\"\"\n",
    "    Count the number of tokens in the text using tiktoken.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to count tokens for\n",
    "        model (str): The tokenizer model to use (default: cl100k_base for GPT-4)\n",
    "\n",
    "    Returns:\n",
    "        int: Number of tokens in the text\n",
    "    \"\"\"\n",
    "    encoder = tiktoken.get_encoding(model)\n",
    "    return len(encoder.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0dbe7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bs4_extractor(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # Target the main article content for LangGraph documentation\n",
    "    main_content = soup.find(\"article\", class_=\"md-content__inner\")\n",
    "\n",
    "    # If found, use that, otherwise fall back to the whole document\n",
    "    content = main_content.get_text() if main_content else soup.text\n",
    "\n",
    "    # Clean up whitespace\n",
    "    content = re.sub(r\"\\n\\n+\", \"\\n\\n\", content).strip()\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b389941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_langgraph_docs():\n",
    "    \"\"\"\n",
    "    Load LangGraph documentation from the official website.\n",
    "\n",
    "    This function:\n",
    "    1. Uses RecursiveUrlLoader to fetch pages from the LangGraph website\n",
    "    2. Counts the total documents and tokens loaded\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Document objects containing the loaded content\n",
    "        list: A list of tokens per document\n",
    "    \"\"\"\n",
    "    print(\"Loading LangGraph documentation...\")\n",
    "\n",
    "    # Load the documentation\n",
    "    urls = [\n",
    "        \"https://langchain-ai.github.io/langgraph/concepts/\",\n",
    "        \"https://langchain-ai.github.io/langgraph/how-tos/\",\n",
    "        \"https://langchain-ai.github.io/langgraph/tutorials/workflows/\",\n",
    "        \"https://langchain-ai.github.io/langgraph/tutorials/introduction/\",\n",
    "        \"https://langchain-ai.github.io/langgraph/tutorials/langgraph-platform/local-server/\",\n",
    "    ]\n",
    "\n",
    "    docs = []\n",
    "    for url in urls:\n",
    "        loader = RecursiveUrlLoader(\n",
    "            url,\n",
    "            max_depth=5,\n",
    "            extractor=bs4_extractor,\n",
    "        )\n",
    "\n",
    "        # Load documents using lazy loading (memory efficient)\n",
    "        docs_lazy = loader.lazy_load()\n",
    "\n",
    "        # Load documents and track URLs\n",
    "        for d in docs_lazy:\n",
    "            docs.append(d)\n",
    "\n",
    "    print(f\"Loaded {len(docs)} documents from LangGraph documentation.\")\n",
    "    print(\"\\nLoaded URLs:\")\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"{i + 1}. {doc.metadata.get('source', 'Unknown URL')}\")\n",
    "\n",
    "    # Count total tokens in documents\n",
    "    total_tokens = 0\n",
    "    tokens_per_doc = []\n",
    "    for doc in docs:\n",
    "        total_tokens += count_tokens(doc.page_content)\n",
    "        tokens_per_doc.append(count_tokens(doc.page_content))\n",
    "    print(f\"Total tokens in loaded documents: {total_tokens}\")\n",
    "\n",
    "    return docs, tokens_per_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a15ed5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_llms_full(documents):\n",
    "    \"\"\"Save the documents to a file\"\"\"\n",
    "\n",
    "    # Open the output file\n",
    "    output_filename = \"llms_full.txt\"\n",
    "\n",
    "    with open(output_filename, \"w\") as f:\n",
    "        # Write each document\n",
    "        for i, doc in enumerate(documents):\n",
    "            # Get the source (URL) from metadata\n",
    "            source = doc.metadata.get(\"source\", \"Unknown URL\")\n",
    "\n",
    "            # Write the document with proper formatting\n",
    "            f.write(f\"DOCUMENT {i + 1}\\n\")\n",
    "            f.write(f\"SOURCE: {source}\\n\")\n",
    "            f.write(\"CONTENT:\\n\")\n",
    "            f.write(doc.page_content)\n",
    "            f.write(\"\\n\\n\" + \"=\" * 80 + \"\\n\\n\")\n",
    "\n",
    "    print(f\"Documents concatenated into {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21f3ebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents):\n",
    "    \"\"\"\n",
    "    Split documents into smaller chunks for improved retrieval.\n",
    "\n",
    "    This function:\n",
    "    1. Uses RecursiveCharacterTextSplitter with tiktoken to create semantically meaningful chunks\n",
    "    2. Ensures chunks are appropriately sized for embedding and retrieval\n",
    "    3. Counts the resulting chunks and their total tokens\n",
    "\n",
    "    Args:\n",
    "        documents (list): List of Document objects to split\n",
    "\n",
    "    Returns:\n",
    "        list: A list of split Document objects\n",
    "    \"\"\"\n",
    "    print(\"Splitting documents...\")\n",
    "\n",
    "    # Initialize text splitter using tiktoken for accurate token counting\n",
    "    # chunk_size=8,000 creates relatively large chunks for comprehensive context\n",
    "    # chunk_overlap=500 ensures continuity between chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=8000, chunk_overlap=500\n",
    "    )\n",
    "\n",
    "    # Split documents into chunks\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    print(f\"Created {len(split_docs)} chunks from documents.\")\n",
    "\n",
    "    # Count total tokens in split documents\n",
    "    total_tokens = 0\n",
    "    for doc in split_docs:\n",
    "        total_tokens += count_tokens(doc.page_content)\n",
    "\n",
    "    print(f\"Total tokens in split documents: {total_tokens}\")\n",
    "\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2af3cd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorstore(splits):\n",
    "    \"\"\"\n",
    "    Create a vector store from document chunks using SKLearnVectorStore.\n",
    "\n",
    "    This function:\n",
    "    1. Initializes an embedding model to convert text into vector representations\n",
    "    2. Creates a vector store from the document chunks\n",
    "\n",
    "    Args:\n",
    "        splits (list): List of split Document objects to embed\n",
    "\n",
    "    Returns:\n",
    "        SKLearnVectorStore: A vector store containing the embedded documents\n",
    "    \"\"\"\n",
    "    print(\"Creating SKLearnVectorStore...\")\n",
    "\n",
    "    # Initialize OpenAI embeddings\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "    # Create vector store from documents using SKLearn\n",
    "    persist_path = os.getcwd() + \"/sklearn_vectorstore.parquet\"\n",
    "    vectorstore = SKLearnVectorStore.from_documents(\n",
    "        documents=splits,\n",
    "        embedding=embeddings,\n",
    "        persist_path=persist_path,\n",
    "        serializer=\"parquet\",\n",
    "    )\n",
    "    print(\"SKLearnVectorStore created successfully.\")\n",
    "\n",
    "    vectorstore.persist()\n",
    "    print(\"SKLearnVectorStore was persisted to\", persist_path)\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85a203f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LangGraph documentation...\n",
      "Loaded 128 documents from LangGraph documentation.\n",
      "\n",
      "Loaded URLs:\n",
      "1. https://langchain-ai.github.io/langgraph/concepts/\n",
      "2. https://langchain-ai.github.io/langgraph/concepts/persistence/\n",
      "3. https://langchain-ai.github.io/langgraph/concepts/high_level/\n",
      "4. https://langchain-ai.github.io/langgraph/concepts/time-travel/\n",
      "5. https://langchain-ai.github.io/langgraph/concepts/functional_api/\n",
      "6. https://langchain-ai.github.io/langgraph/concepts/v0-human-in-the-loop/\n",
      "7. https://langchain-ai.github.io/langgraph/concepts/memory/\n",
      "8. https://langchain-ai.github.io/langgraph/concepts/durable_execution/\n",
      "9. https://langchain-ai.github.io/langgraph/concepts/streaming/\n",
      "10. https://langchain-ai.github.io/langgraph/concepts/low_level/\n",
      "11. https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/\n",
      "12. https://langchain-ai.github.io/langgraph/concepts/multi_agent/\n",
      "13. https://langchain-ai.github.io/langgraph/concepts/breakpoints/\n",
      "14. https://langchain-ai.github.io/langgraph/concepts/faq/\n",
      "15. https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/\n",
      "16. https://langchain-ai.github.io/langgraph/concepts/pregel/\n",
      "17. https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/\n",
      "18. https://langchain-ai.github.io/langgraph/concepts/langgraph_server/\n",
      "19. https://langchain-ai.github.io/langgraph/concepts/assistants/\n",
      "20. https://langchain-ai.github.io/langgraph/concepts/sdk/\n",
      "21. https://langchain-ai.github.io/langgraph/concepts/langgraph_data_plane/\n",
      "22. https://langchain-ai.github.io/langgraph/concepts/langgraph_cli/\n",
      "23. https://langchain-ai.github.io/langgraph/concepts/langgraph_control_plane/\n",
      "24. https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/\n",
      "25. https://langchain-ai.github.io/langgraph/concepts/langgraph_self_hosted_data_plane/\n",
      "26. https://langchain-ai.github.io/langgraph/concepts/langgraph_cloud/\n",
      "27. https://langchain-ai.github.io/langgraph/concepts/langgraph_standalone_container/\n",
      "28. https://langchain-ai.github.io/langgraph/concepts/self_hosted/\n",
      "29. https://langchain-ai.github.io/langgraph/concepts/langgraph_self_hosted_control_plane/\n",
      "30. https://langchain-ai.github.io/langgraph/concepts/auth/\n",
      "31. https://langchain-ai.github.io/langgraph/concepts/application_structure/\n",
      "32. https://langchain-ai.github.io/langgraph/concepts/double_texting/\n",
      "33. https://langchain-ai.github.io/langgraph/concepts/platform_architecture/\n",
      "34. https://langchain-ai.github.io/langgraph/concepts/plans/\n",
      "35. https://langchain-ai.github.io/langgraph/concepts/deployment_options/\n",
      "36. https://langchain-ai.github.io/langgraph/concepts/bring_your_own_cloud/\n",
      "37. https://langchain-ai.github.io/langgraph/concepts/template_applications/\n",
      "38. https://langchain-ai.github.io/langgraph/concepts/scalability_and_resilience/\n",
      "39. https://langchain-ai.github.io/langgraph/concepts/plans/&\n",
      "40. https://langchain-ai.github.io/langgraph/how-tos/\n",
      "41. https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/review-tool-calls/\n",
      "42. https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/dynamic_breakpoints/\n",
      "43. https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/wait-user-input/\n",
      "44. https://langchain-ai.github.io/langgraph/how-tos/review-tool-calls-functional/\n",
      "45. https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/breakpoints/\n",
      "46. https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/time-travel/\n",
      "47. https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/edit-graph-state/\n",
      "48. https://langchain-ai.github.io/langgraph/how-tos/wait-user-input-functional/\n",
      "49. https://langchain-ai.github.io/langgraph/how-tos/autogen-langgraph-platform/\n",
      "50. https://langchain-ai.github.io/langgraph/how-tos/ttl/configure_ttl/\n",
      "51. https://langchain-ai.github.io/langgraph/how-tos/auth/custom_auth/\n",
      "52. https://langchain-ai.github.io/langgraph/how-tos/auth/openapi_security/\n",
      "53. https://langchain-ai.github.io/langgraph/how-tos/configuration/\n",
      "54. https://langchain-ai.github.io/langgraph/how-tos/map-reduce/\n",
      "55. https://langchain-ai.github.io/langgraph/how-tos/node-retries/\n",
      "56. https://langchain-ai.github.io/langgraph/how-tos/return-when-recursion-limit-hits/\n",
      "57. https://langchain-ai.github.io/langgraph/how-tos/command/\n",
      "58. https://langchain-ai.github.io/langgraph/how-tos/visualization/\n",
      "59. https://langchain-ai.github.io/langgraph/how-tos/state-reducers/\n",
      "60. https://langchain-ai.github.io/langgraph/how-tos/recursion-limit/\n",
      "61. https://langchain-ai.github.io/langgraph/how-tos/branching/\n",
      "62. https://langchain-ai.github.io/langgraph/how-tos/sequence/\n",
      "63. https://langchain-ai.github.io/langgraph/how-tos/subgraph-persistence/\n",
      "64. https://langchain-ai.github.io/langgraph/how-tos/persistence/\n",
      "65. https://langchain-ai.github.io/langgraph/how-tos/cross-thread-persistence/\n",
      "66. https://langchain-ai.github.io/langgraph/how-tos/persistence_postgres/\n",
      "67. https://langchain-ai.github.io/langgraph/how-tos/persistence_redis/\n",
      "68. https://langchain-ai.github.io/langgraph/how-tos/persistence-functional/\n",
      "69. https://langchain-ai.github.io/langgraph/how-tos/cross-thread-persistence-functional/\n",
      "70. https://langchain-ai.github.io/langgraph/how-tos/persistence_mongodb/\n",
      "71. https://langchain-ai.github.io/langgraph/how-tos/subgraph\n",
      "72. https://langchain-ai.github.io/langgraph/how-tos/subgraph/\n",
      "73. https://langchain-ai.github.io/langgraph/how-tos/many-tools/\n",
      "74. https://langchain-ai.github.io/langgraph/how-tos/subgraph-transform-state/\n",
      "75. https://langchain-ai.github.io/langgraph/how-tos/streaming-subgraphs/\n",
      "76. https://langchain-ai.github.io/langgraph/how-tos/subgraphs-manage-state/\n",
      "77. https://langchain-ai.github.io/langgraph/how-tos/streaming/\n",
      "78. https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens\n",
      "79. https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/\n",
      "80. https://langchain-ai.github.io/langgraph/how-tos/streaming-events-from-within-tools/\n",
      "81. https://langchain-ai.github.io/langgraph/how-tos/streaming-specific-nodes/\n",
      "82. https://langchain-ai.github.io/langgraph/how-tos/disable-streaming/\n",
      "83. https://langchain-ai.github.io/langgraph/how-tos/memory/delete-messages/\n",
      "84. https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/\n",
      "85. https://langchain-ai.github.io/langgraph/how-tos/memory/delete-messages\n",
      "86. https://langchain-ai.github.io/langgraph/how-tos/manage-conversation-history/\n",
      "87. https://langchain-ai.github.io/langgraph/how-tos/semantic-search/\n",
      "88. https://langchain-ai.github.io/langgraph/how-tos/add-summary-conversation-history/\n",
      "89. https://langchain-ai.github.io/langgraph/how-tos/memory/\n",
      "90. https://langchain-ai.github.io/langgraph/how-tos/memory/semantic-search/\n",
      "91. https://langchain-ai.github.io/langgraph/how-tos/memory/manage-conversation-history/\n",
      "92. https://langchain-ai.github.io/langgraph/how-tos/http/custom_lifespan/\n",
      "93. https://langchain-ai.github.io/langgraph/how-tos/http/custom_middleware/\n",
      "94. https://langchain-ai.github.io/langgraph/how-tos/http/custom_routes/\n",
      "95. https://langchain-ai.github.io/langgraph/how-tos/react-agent-structured-output/\n",
      "96. https://langchain-ai.github.io/langgraph/how-tos/async/\n",
      "97. https://langchain-ai.github.io/langgraph/how-tos/autogen-integration-functional/\n",
      "98. https://langchain-ai.github.io/langgraph/how-tos/run-id-langsmith/\n",
      "99. https://langchain-ai.github.io/langgraph/how-tos/autogen-integration/\n",
      "100. https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/\n",
      "101. https://langchain-ai.github.io/langgraph/how-tos/pass_private_state/\n",
      "102. https://langchain-ai.github.io/langgraph/how-tos/state-model/\n",
      "103. https://langchain-ai.github.io/langgraph/how-tos/input_output_schema/\n",
      "104. https://langchain-ai.github.io/langgraph/how-tos/create-react-agent-hitl/\n",
      "105. https://langchain-ai.github.io/langgraph/how-tos/create-react-agent-structured-output/\n",
      "106. https://langchain-ai.github.io/langgraph/how-tos/react-agent-structured-output\n",
      "107. https://langchain-ai.github.io/langgraph/how-tos/react-agent-from-scratch-functional/\n",
      "108. https://langchain-ai.github.io/langgraph/how-tos/create-react-agent-system-prompt/\n",
      "109. https://langchain-ai.github.io/langgraph/how-tos/create-react-agent-manage-message-history/\n",
      "110. https://langchain-ai.github.io/langgraph/how-tos/create-react-agent-memory/\n",
      "111. https://langchain-ai.github.io/langgraph/how-tos/react-agent-from-scratch/\n",
      "112. https://langchain-ai.github.io/langgraph/how-tos/create-react-agent\n",
      "113. https://langchain-ai.github.io/langgraph/how-tos/multi-agent-network-functional/\n",
      "114. https://langchain-ai.github.io/langgraph/how-tos/multi-agent-network/\n",
      "115. https://langchain-ai.github.io/langgraph/how-tos/multi-agent-multi-turn-convo/\n",
      "116. https://langchain-ai.github.io/langgraph/how-tos/agent-handoffs\n",
      "117. https://langchain-ai.github.io/langgraph/how-tos/agent-handoffs/\n",
      "118. https://langchain-ai.github.io/langgraph/how-tos/multi-agent-multi-turn-convo-functional/\n",
      "119. https://langchain-ai.github.io/langgraph/how-tos/use-remote-graph/\n",
      "120. https://langchain-ai.github.io/langgraph/how-tos/tool-calling-errors/\n",
      "121. https://langchain-ai.github.io/langgraph/how-tos/update-state-from-tools/\n",
      "122. https://langchain-ai.github.io/langgraph/how-tos/tool-calling/\n",
      "123. https://langchain-ai.github.io/langgraph/how-tos/pass-config-to-tools/\n",
      "124. https://langchain-ai.github.io/langgraph/how-tos/pass-run-time-values-to-tools/\n",
      "125. https://langchain-ai.github.io/langgraph/how-tos/local-studio/\n",
      "126. https://langchain-ai.github.io/langgraph/tutorials/workflows/\n",
      "127. https://langchain-ai.github.io/langgraph/tutorials/introduction/\n",
      "128. https://langchain-ai.github.io/langgraph/tutorials/langgraph-platform/local-server/\n",
      "Total tokens in loaded documents: 305857\n",
      "Documents concatenated into llms_full.txt\n",
      "Splitting documents...\n",
      "Created 141 chunks from documents.\n",
      "Total tokens in split documents: 308981\n",
      "Creating SKLearnVectorStore...\n",
      "SKLearnVectorStore created successfully.\n",
      "SKLearnVectorStore was persisted to /home/mori/learning-langchain/mcp/sklearn_vectorstore.parquet\n"
     ]
    }
   ],
   "source": [
    "# Load the documents\n",
    "documents, tokens_per_doc = load_langgraph_docs()\n",
    "\n",
    "# Save the documents to a file\n",
    "save_llms_full(documents)\n",
    "\n",
    "# Split the documents\n",
    "split_docs = split_documents(documents)\n",
    "\n",
    "# Create the vector store\n",
    "vectorstore = create_vectorstore(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d61ea600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 3 relevant documents\n",
      "https://langchain-ai.github.io/langgraph/concepts/high_level/\n",
      "Why LangGraph?¶\n",
      "LLM applications¶\n",
      "LLMs make it possible to embed intelligence into a new class of applications. There are many patterns for building applications that use LLMs. Workflows have scaffold\n",
      "\n",
      "--------------------------------\n",
      "\n",
      "https://langchain-ai.github.io/langgraph/concepts/high_level/\n",
      "Why LangGraph?¶\n",
      "LLM applications¶\n",
      "LLMs make it possible to embed intelligence into a new class of applications. There are many patterns for building applications that use LLMs. Workflows have scaffold\n",
      "\n",
      "--------------------------------\n",
      "\n",
      "https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/\n",
      "LangGraph Platform¶\n",
      "Watch this 4-minute overview of LangGraph Platform to see how it helps you build, deploy, and evaluate agentic applications.\n",
      "\n",
      "Overview¶\n",
      "LangGraph Platform is a commercial solution \n",
      "\n",
      "--------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "query = \"LangGraphとはなんですか？\"\n",
    "relevant_docs = retriever.invoke(query)\n",
    "print(f\"Retrieved {len(relevant_docs)} relevant documents\")\n",
    "\n",
    "for d in relevant_docs:\n",
    "    print(d.metadata[\"source\"])\n",
    "    print(d.page_content[0:200])\n",
    "    print(\"\\n--------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9740b3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def langgraph_query_tool(query: str):\n",
    "    \"\"\"\n",
    "    Query the LangGraph documentation using a retriever.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query to search the documentation with\n",
    "\n",
    "    Returns:\n",
    "        str: A str of the retrieved documents\n",
    "    \"\"\"\n",
    "    retriever = SKLearnVectorStore(\n",
    "        embedding=OpenAIEmbeddings(model=\"text-embedding-3-large\"),\n",
    "        persist_path=os.getcwd() + \"/sklearn_vectorstore.parquet\",\n",
    "        serializer=\"parquet\",\n",
    "    ).as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "    relevant_docs = retriever.invoke(query)\n",
    "    print(f\"Retrieved {len(relevant_docs)} relevant documents\")\n",
    "    formatted_context = \"\\n\\n\".join(\n",
    "        [\n",
    "            f\"==DOCUMENT {i + 1}==\\n{doc.page_content}\"\n",
    "            for i, doc in enumerate(relevant_docs)\n",
    "        ]\n",
    "    )\n",
    "    return formatted_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d692a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "[{'text': 'LangGraphの特徴について調べてみますね。LangGraphのドキュメントから情報を取得します。', 'type': 'text'}, {'id': 'toolu_01E6UT2vxs9XRM1kszyuMY6F', 'input': {'query': 'LangGraph features characteristics'}, 'name': 'langgraph_query_tool', 'type': 'tool_use'}]\n",
      "Tool Calls:\n",
      "  langgraph_query_tool (toolu_01E6UT2vxs9XRM1kszyuMY6F)\n",
      " Call ID: toolu_01E6UT2vxs9XRM1kszyuMY6F\n",
      "  Args:\n",
      "    query: LangGraph features characteristics\n"
     ]
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "\n",
    "llm = ChatAnthropic(model=\"claude-3-7-sonnet-latest\", temperature=0)\n",
    "augmented_llm = llm.bind_tools([langgraph_query_tool])\n",
    "\n",
    "instructions = \"\"\"You are a helpful assistant that can answer questions about the LangGraph documentation. \n",
    "Use the langgraph_query_tool for any questions about the documentation.\n",
    "If you don't know the answer, say \"I don't know.\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": instructions},\n",
    "    {\"role\": \"user\", \"content\": \"LangGraphの特徴はなんですか？\"},\n",
    "]\n",
    "\n",
    "message = augmented_llm.invoke(messages)\n",
    "message.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
