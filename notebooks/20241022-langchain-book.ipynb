{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LangChain解説"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.0)\n",
    "\n",
    "result = llm.invoke(\"自己紹介してください。\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"こんにちは！私はジョンと言います！\"),\n",
    "    AIMessage(content=\"こんにちは、ジョンさん！どのようにお手伝いできますか？\"),\n",
    "    HumanMessage(content= \"私の名前が分かりますか？\")\n",
    "]\n",
    "\n",
    "result = chat.invoke(messages)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "messages = [HumanMessage(content=\"自己紹介してください。\")]\n",
    "\n",
    "for chunk in chat.stream(messages):\n",
    "    print(chunk.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"以下のレシピを考えてください。\n",
    "\n",
    "料理名: {dish}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "result = prompt.format(dish=\"カレー\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"あなたは{country}料理のプロフェッショナルです。\"),\n",
    "    (\"human\", \"以下の料理のレシピを考えてください。\\n\\n料理名: {dish}\")\n",
    "])\n",
    "\n",
    "messages = chat_prompt.format_messages(country=\"イギリス\", dish=\"肉じゃが\")\n",
    "\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.0)\n",
    "result = chat.invoke(messages)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    ingredients: list[str] = Field(description=\"ingredients of the dish\")\n",
    "    steps: list[str] = Field(description=\"steps to make the dish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"以下の料理のレシピを考えてください。\\n\\n料理名: {dish}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.0).with_structured_output(Recipe)\n",
    "\n",
    "chain = prompt | chat\n",
    "\n",
    "output = chain.invoke({\"dish\": \"カレー\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.0)\n",
    "\n",
    "cot_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"以下の質問に回答してください。\n",
    "\n",
    "質問: {question}\n",
    "\n",
    "ステップバイステップで考えましょう。\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "cot_chain = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | cot_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "summarize_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"以下の文章の結論だけを一言に要約してください。\n",
    "\n",
    "{input}\"\"\"\n",
    ")\n",
    "\n",
    "summarize_chain = (\n",
    "    {\"input\": RunnablePassthrough()}\n",
    "    | summarize_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "cot_summarize_chain = cot_chain | summarize_chain\n",
    "\n",
    "result = cot_summarize_chain.invoke(\n",
    "    \"私は市場に行って10個のリンゴを買いました。隣人に2つ、修理工に2つ渡しました。それから5つのリンゴを買って1つ食べました。残りは何個ですか？\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
    "    )\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "while True:\n",
    "    user_message = input(\"You: \")\n",
    "    inputs = {\"input\": user_message}\n",
    "\n",
    "    ai_message = chain.invoke(inputs)\n",
    "    memory.save_context(inputs, {\"output\": ai_message})\n",
    "\n",
    "    print(ai_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChainの活用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document loaders\n",
    "from langchain.document_loaders import GitLoader\n",
    "\n",
    "\n",
    "def file_filter(file_path):\n",
    "    return file_path.endswith(\".mdx\")\n",
    "\n",
    "\n",
    "loader = GitLoader(\n",
    "    clone_url=\"https://github.com/langchain-ai/langchain\",\n",
    "    repo_path=\"./langchain\",\n",
    "    branch=\"master\",\n",
    "    file_filter=file_filter,\n",
    ")\n",
    "\n",
    "raw_docs = loader.load()\n",
    "print(len(raw_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document transformers\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(raw_docs)\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text embedding models\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"AWSのS3からデータを読み込むためのDocumentLoaderはありますか？\"\n",
    "\n",
    "vector = embeddings.embed_query(query)\n",
    "print(len(vector))\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector stores\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "db = Chroma.from_documents(docs, embeddings)\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"AWSのS3からデータを読み込むためのDocumentLoaderはありますか？\"\n",
    "\n",
    "context_docs = retriever.invoke(query)\n",
    "print(f\"len = {len(context_docs)}\")\n",
    "\n",
    "first_doc = context_docs[0]\n",
    "print(f\"metadata = {first_doc.metadata}\")\n",
    "print(first_doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCEL RAG\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"以下の文脈だけをふまえて質問に回答してください。\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\")\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = chain.invoke(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.globals import set_verbose\n",
    "\n",
    "set_verbose(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent, load_tools\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.0)\n",
    "tools = load_tools([\"terminal\"], allow_dangerous_tools=True)\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "agent = create_react_agent(chat, tools, prompt)\n",
    "\n",
    "agent_chain = AgentExecutor(agent=agent, tools=tools)\n",
    "\n",
    "result = agent_chain.invoke({\"input\": \"このディレクトリにあるファイルの一覧を教えて\"})\n",
    "print(result[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import Tool\n",
    "\n",
    "def my_super_func(param):\n",
    "    return \"42\"\n",
    "\n",
    "tools = [\n",
    "    Tool.from_function(\n",
    "        func=my_super_func,\n",
    "        name=\"The_Answer\",\n",
    "        description=\"生命、宇宙、そして万物についての究極の疑問の答え\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_react_agent(chat, tools, prompt)\n",
    "agent_chain = AgentExecutor(agent=agent, tools=tools)\n",
    "\n",
    "result = agent_chain.invoke({\"input\": \"この世界の真理を教えてください\"})\n",
    "print(result[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "summarize_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"以下の文章を結論だけ一言に要約してください。\n",
    "\n",
    "{input}\"\"\"\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.0)\n",
    "\n",
    "summarize_chain = (\n",
    "    {\"input\": RunnablePassthrough()}\n",
    "    | summarize_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "tools = [\n",
    "    Tool.from_function(\n",
    "        func=summarize_chain.invoke,\n",
    "        name=\"Summarizer\",\n",
    "        description=\"Text summarizer\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_chain.invoke({\"input\": \"これはテストの文章です\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_react_agent(chat, tools, prompt)\n",
    "agent_chain = AgentExecutor(agent=agent, tools=tools)\n",
    "\n",
    "text = \"\"\"以下を要約してください。\n",
    "\n",
    "こんにちは！私はChatGPTと呼ばれるAI言語モデルです。OpenAIが開発したGPT-3.5アーキテクチャに基づいています。私は自然言語理解と生成に特化しており、さまざまなトピックに関する質問に答えたり、おしゃべりしたりすることが得意です。\n",
    "私のトレーニングデータは2021年9月までの情報に基づいているため、それ以降の出来事については知識がありません。ですが、できる限りお手伝いすることに努めます。\n",
    "質問や会話、情報の共有など、どんなお手伝いでもお気軽にお申し付けください！よろしくお願いします。\"\"\"\n",
    "\n",
    "result = agent_chain.invoke({\"input\": text})\n",
    "print(result[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function callingを使うOpenAI Functions Agent\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent, load_tools\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "tools = load_tools([\"terminal\"], allow_dangerous_tools=True)\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "\n",
    "agent = create_openai_tools_agent(chat, tools, prompt)\n",
    "agent_chain = AgentExecutor(agent=agent, tools=tools)\n",
    "\n",
    "result = agent_chain.invoke({\"input\": \"このディレクトリにあるファイルの一覧を教えて\"})\n",
    "print(result[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 複数のツールを一度に使うOpenAI Multi Functions Agent\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent, load_tools\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "tools = load_tools([\"ddg-search\"])\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "\n",
    "agent = create_openai_tools_agent(chat, tools, prompt)\n",
    "agent_chain = AgentExecutor(agent=agent, tools=tools)\n",
    "\n",
    "result = agent_chain.invoke({\"input\": \"東京と大阪の天気を教えて\"})\n",
    "print(result[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function callingを応用したOutputParser\n",
    "import json\n",
    "from typing import Optional\n",
    "\n",
    "from langchain.output_parsers.openai_tools import JsonOutputToolsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Person(BaseModel):\n",
    "    person_name: str\n",
    "    person_height: str\n",
    "    person_hair_color: Optional[str]\n",
    "    dog_name: Optional[str]\n",
    "    dog_breed: Optional[str]\n",
    "\n",
    "class People(BaseModel):\n",
    "    people: list[Person]\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.0).bind_tools([People])\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpful assistant.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | model | JsonOutputToolsParser()\n",
    "\n",
    "text = \"\"\"\n",
    "Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.\n",
    "Alex's dog Frosty is a labrador and likes to play hide and seek.\n",
    "\"\"\"\n",
    "\n",
    "people = chain.invoke({\"input\": text})\n",
    "print(people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(people, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "from langchain.evaluation import load_evaluator\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "\n",
    "evaluator = load_evaluator(\"qa\", eval_llm=chat)\n",
    "\n",
    "result = evaluator.evaluate_strings(\n",
    "    input=\"私は市場に行って10個のリンゴを買いました。隣人に2つ、修理工に2つ渡しました。それから5つのリンゴを買って1つ食べました。残りは何個ですか？\",\n",
    "    prediction=\"\"\"1最初に10個のリンゴを買い、その中から隣人と修理工にそれぞれ2個ずつ渡しました。そのため、まず手元に残ったリンゴは10 - 2 - 2 = 6個となります。\n",
    "\n",
    "その後、さらに5個のリンゴを買い、1つ食べました。これにより手元のリンゴは6 + 5 - 1 = 10個となります。\"\"\",\n",
    "    reference=\"10個\",\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning-langchain-BRoTc1ZH-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
