{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part1: Overview\n",
    "\n",
    "- https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_1_to_4.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://zenn.dev/knowledgesense/articles/47de9ead8029ba', 'title': 'Zenn'}, page_content='Zennナレッジセンス - AI知見共有ブログナレッジセンス - AI知見共有ブログPublicationへの投稿🤖RAGとは？回答精度向上のためのテクニック集（基礎編）Atsushi Kadowaki2024/02/27に公開2024/03/09エンジニアチャットボットChatGPT生成 AIRAGtechはじめまして。株式会社ナレッジセンスの門脇です。普段はエンジニア兼PMとして、「社内データに基づいて回答してくれる」チャットボットをエンタープライズ企業向けに提供しています（一応、200社以上に導入実績あり）。ここで開発しているチャットボットは、ChatGPTを始めとしたLLM（Large Language Models）を活用したサービスであり、その中でもRAG（Retrieval Augmented Generative）という仕組みをガッツリ利用しています。本記事では、RAG精度向上のための知見を共有していきます。\\n\\n\\n はじめに\\n\\n この記事は何\\nこの記事は、LlamaIndexのAndrei氏による『A Cheat Sheet and Some Recipes For Building Advanced RAG』[1]という記事で紹介されている「RAGに関するチートシート」について、Andrei氏の許可を得て翻訳し、解説したものです。（※なお、このAndrei氏のチートシート自体、Y Gao et al.によるサーベイ論文[2]に多大な影響を受けています）\\nこのチートシートを見たとき、「自分が実務でやっていることそのものだ」と感銘を受けたのを覚えています。とても有益なシートです。\\n元記事で紹介されているチートシートは非常に膨大だったため、何度かに分割して紹介できればと思います。\\n本記事は「基礎編」です。いわゆるAdvancedなテクニックは未だ出てきません。まずはエンジニアが、周辺のステークホルダーを説得するためのチートシートという感じに位置づけられそうです。\\n\\n 対象読者\\n\\n生成AIに取り組むエンジニア。\\n生成AIに関わるビジネスサイドの方。\\n\\n特にこの「基礎編」のチートシートは「エンジニアがビジネスサイドに説明するため」に有効活用できる内容だと思います。ご自身が理解するためにはもちろん、社内やクライアントにRAGについて説明するとき、必ず役に立つと思います。この基礎編のチートシートは、「RAGを全く知らない人に説明するときに伝えるべき全て」が必要十分に記載されているという印象です。\\n\\n 本題\\n\\n RAGテクニック集（チートシート）\\n以下はチートシートです。元記事全体のチートシートのうち、「基礎編」に該当する部分だけ切り出しています。以下、更に細かく分けてこのシートを補足していきます。\\n\\n\\n RAGとは。なぜ必要なのか？\\nRAGとは、大雑把に言うと、ファイルを参照して回答できるLLM（例えばChatGPT）を作成するための方法です。RAGがなぜ必要なのかというと、通常のLLMでは、回答の正確性を向上させるのに限界があるからです。通常のLLMでは、事実と違う内容を勝手に捏造してしまったり（ハルシネーション）、そもそも学習データに含まれていない情報（例えば公開されていない社内の文書）については、回答することはできなかったりという限界があります。\\n\\n\\n RAGの仕組み\\nこの記事をご覧のエンジニアの方であれば、既にご存知の内容かと思います。以下、チートシートの引用です。\\n「RAGでは、ユーザーが質問すると、まず外部データベースから関連するドキュメントを取得する。そのドキュメントと元々のユーザーの質問がセットされ、LLMに渡される。LLMは、この内容をもとに回答を生成する」\\n非常にシンプルな内容なので、この図と一緒に説明すれば、ビジネスサイドの方でも理解してもらえます。私個人的にも、似たような図を使って顧客や社内ビジネスサイドに説明していて、必ず理解してもらえる印象です。\\n\\nRAGの良さはこのシンプルさ、始めやすさなのですが、始めのうちは、なかなか思い通りの回答が得られません。この回答精度を上げようとすると、かなりの苦難が待っています...\\n\\n RAGの精度向上のために必要なこと\\nRAGの精度向上を試みる際に重要な要素は、以下の2点に分解できます。\\n\\nユーザーの質問に回答するために最も必要な（最も関連している）ドキュメント群を抽出すること\\n抽出してきたドキュメント群を最大限上手く活用して、正しい回答を生成すること\\n\\nこの2点は、上の「RAGの仕組み」で登場した画像の中の黄色い枠で囲まれている部分に該当します。\\n※具体的な手法は、今後の記事で紹介していきます\\n\\n RAGを使ってできること\\n\\n個人的には、実務でRAGを使っていて嬉しいポイントは\\n\\n情報不十分なとき、回答しないことができる\\n独自のドキュメントに基づいて回答できる（上の画像には含まれていませんが）\\n\\nことです。画像中で4つ目に列挙されている「情報ソース自体の間違いを指摘できる」というのは使い所が分かりづらいですが、コード中のバグを探す、というようなユースケースが想定されてそうです。\\nまた、画像中の1つ目にある「ノイズを減らせる」という点については、「LLMは、人間の質問が長いほど回答の精度が落ちる」（注[3]）という点を前提にしています。大量の文章をLLMに解釈させるのではなく、関係あるだけ読んでもらって回答精度を向上させようよ、ということが可能になります。また、これはすなわち「金銭的なコストを節約できる」というメリットにも繋がります。大抵のLLMは、AIへの入力とAIからの出力の文字数で課金されるからです。\\n\\n RAGの品質計測\\n\\n「改善は計測から」なので、どのように評価するかは改善前に決めておく必要があります。\\n補足として、こちらの評価指標については、RAG評価ツールの「RAGAS」から、もう少し多くの指標が提案されています。[4]\\n\\n まとめ\\nRAGのテクニックを紹介すると言いながら、この「基礎編」ではいわゆるAdvancedなテクニックは紹介できなかったのが心残りです。（今後の続編にて掲載するためお許しください🙏）\\nただ個人的には、エンジニア向けのチートシートの中にガッツリ、この内容が含まれていることに意味があると思っていて、それは、「ステークホルダーを説得することも、エンジニアが身につけるべきテクニックである」ということだと思うからです。なので、このチートシートを早速社内で展開しています。\\n直近で続編を出す予定で、そこでは「チャンキング」や「ナレッジグラフ」などの手法に触れる予定です。（待てない方は、私の過去の投稿を御覧ください。割と内容か被りそうです...）\\n\\n\\n脚注\\n\\n\\n『A Cheat Sheet and Some Recipes For Building Advanced RAG』 ↩︎\\n\\n\\nRetrieval-Augmented Generation for Large Language Models: A Survey ↩︎\\n\\n\\n最近では、Gemini 1.5など、大量の文章を投下しても精度が落ちづらいモデルも登場してくる流れになっています。（リンク） ↩︎\\n\\n\\nhttps://docs.ragas.io/en/stable/concepts/metrics/index.html ↩︎\\n\\n\\n\\nAtsushi Kadowakiナレッジセンス CEO ← 東大  / エンタープライズ向け生成AIプロダクトで成長中のスタートアップ（2019年~） / ソフトウェアエンジニアを募集中（800万円~）→DM開放中 / 好きな言葉は「実験と学習」/ 最新の生成AI 事情に少し詳しいですナレッジセンス - AI知見共有ブログPublication株式会社ナレッジセンスは、「大企業の知的活動を最速にする」をミッションに掲げ、社内データ検索ができるAIチャットボットを開発・提供しているスタートアップです。このブログでは、LLMや検索技術、RAGの実装戦略などについて知見を共有していきます。DiscussionAtsushi Kadowakiナレッジセンス CEO ← 東大  / エンタープライズ向け生成AIプロダクトで成長中のスタートアップ（2019年~） / ソフトウェアエンジニアを募集中（800万円~）→DM開放中 / 好きな言葉は「実験と学習」/ 最新の生成AI 事情に少し詳しいです目次はじめにこの記事は何対象読者本題RAGテクニック集（チートシート）RAGとは。なぜ必要なのか？RAGの仕組みRAGの精度向上のために必要なことRAGを使ってできることRAGの品質計測まとめZennエンジニアのための情報共有コミュニティAboutZennについて運営会社お知らせ・リリースGuides使い方法人向けメニューNewPublication / Proよくある質問LinksX(Twitter)GitHubメディアキットLegal利用規約プライバシーポリシー特商法表記')]\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://zenn.dev/knowledgesense/articles/47de9ead8029ba\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"Container_wide__ykGLh Container_common__figYY\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://zenn.dev/knowledgesense/articles/47de9ead8029ba', 'title': 'Zenn'}, page_content='Zennナレッジセンス - AI知見共有ブログナレッジセンス - AI知見共有ブログPublicationへの投稿🤖RAGとは？回答精度向上のためのテクニック集（基礎編）Atsushi Kadowaki2024/02/27に公開2024/03/09エンジニアチャットボットChatGPT生成 AIRAGtechはじめまして。株式会社ナレッジセンスの門脇です。普段はエンジニア兼PMとして、「社内データに基づいて回答してくれる」チャットボットをエンタープライズ企業向けに提供しています（一応、200社以上に導入実績あり）。ここで開発しているチャットボットは、ChatGPTを始めとしたLLM（Large Language Models）を活用したサービスであり、その中でもRAG（Retrieval Augmented Generative）という仕組みをガッツリ利用しています。本記事では、RAG精度向上のための知見を共有していきます。\\n\\n\\n はじめに'),\n",
       " Document(metadata={'source': 'https://zenn.dev/knowledgesense/articles/47de9ead8029ba', 'title': 'Zenn'}, page_content='この記事は何\\nこの記事は、LlamaIndexのAndrei氏による『A Cheat Sheet and Some Recipes For Building Advanced RAG』[1]という記事で紹介されている「RAGに関するチートシート」について、Andrei氏の許可を得て翻訳し、解説したものです。（※なお、このAndrei氏のチートシート自体、Y Gao et al.によるサーベイ論文[2]に多大な影響を受けています）\\nこのチートシートを見たとき、「自分が実務でやっていることそのものだ」と感銘を受けたのを覚えています。とても有益なシートです。\\n元記事で紹介されているチートシートは非常に膨大だったため、何度かに分割して紹介できればと思います。\\n本記事は「基礎編」です。いわゆるAdvancedなテクニックは未だ出てきません。まずはエンジニアが、周辺のステークホルダーを説得するためのチートシートという感じに位置づけられそうです。\\n\\n 対象読者\\n\\n生成AIに取り組むエンジニア。\\n生成AIに関わるビジネスサイドの方。'),\n",
       " Document(metadata={'source': 'https://zenn.dev/knowledgesense/articles/47de9ead8029ba', 'title': 'Zenn'}, page_content='特にこの「基礎編」のチートシートは「エンジニアがビジネスサイドに説明するため」に有効活用できる内容だと思います。ご自身が理解するためにはもちろん、社内やクライアントにRAGについて説明するとき、必ず役に立つと思います。この基礎編のチートシートは、「RAGを全く知らない人に説明するときに伝えるべき全て」が必要十分に記載されているという印象です。\\n\\n 本題\\n\\n RAGテクニック集（チートシート）\\n以下はチートシートです。元記事全体のチートシートのうち、「基礎編」に該当する部分だけ切り出しています。以下、更に細かく分けてこのシートを補足していきます。'),\n",
       " Document(metadata={'source': 'https://zenn.dev/knowledgesense/articles/47de9ead8029ba', 'title': 'Zenn'}, page_content='RAGとは。なぜ必要なのか？\\nRAGとは、大雑把に言うと、ファイルを参照して回答できるLLM（例えばChatGPT）を作成するための方法です。RAGがなぜ必要なのかというと、通常のLLMでは、回答の正確性を向上させるのに限界があるからです。通常のLLMでは、事実と違う内容を勝手に捏造してしまったり（ハルシネーション）、そもそも学習データに含まれていない情報（例えば公開されていない社内の文書）については、回答することはできなかったりという限界があります。\\n\\n\\n RAGの仕組み\\nこの記事をご覧のエンジニアの方であれば、既にご存知の内容かと思います。以下、チートシートの引用です。\\n「RAGでは、ユーザーが質問すると、まず外部データベースから関連するドキュメントを取得する。そのドキュメントと元々のユーザーの質問がセットされ、LLMに渡される。LLMは、この内容をもとに回答を生成する」\\n非常にシンプルな内容なので、この図と一緒に説明すれば、ビジネスサイドの方でも理解してもらえます。私個人的にも、似たような図を使って顧客や社内ビジネスサイドに説明していて、必ず理解してもらえる印象です。'),\n",
       " Document(metadata={'source': 'https://zenn.dev/knowledgesense/articles/47de9ead8029ba', 'title': 'Zenn'}, page_content='RAGの良さはこのシンプルさ、始めやすさなのですが、始めのうちは、なかなか思い通りの回答が得られません。この回答精度を上げようとすると、かなりの苦難が待っています...\\n\\n RAGの精度向上のために必要なこと\\nRAGの精度向上を試みる際に重要な要素は、以下の2点に分解できます。\\n\\nユーザーの質問に回答するために最も必要な（最も関連している）ドキュメント群を抽出すること\\n抽出してきたドキュメント群を最大限上手く活用して、正しい回答を生成すること\\n\\nこの2点は、上の「RAGの仕組み」で登場した画像の中の黄色い枠で囲まれている部分に該当します。\\n※具体的な手法は、今後の記事で紹介していきます\\n\\n RAGを使ってできること\\n\\n個人的には、実務でRAGを使っていて嬉しいポイントは\\n\\n情報不十分なとき、回答しないことができる\\n独自のドキュメントに基づいて回答できる（上の画像には含まれていませんが）'),\n",
       " Document(metadata={'source': 'https://zenn.dev/knowledgesense/articles/47de9ead8029ba', 'title': 'Zenn'}, page_content='ことです。画像中で4つ目に列挙されている「情報ソース自体の間違いを指摘できる」というのは使い所が分かりづらいですが、コード中のバグを探す、というようなユースケースが想定されてそうです。\\nまた、画像中の1つ目にある「ノイズを減らせる」という点については、「LLMは、人間の質問が長いほど回答の精度が落ちる」（注[3]）という点を前提にしています。大量の文章をLLMに解釈させるのではなく、関係あるだけ読んでもらって回答精度を向上させようよ、ということが可能になります。また、これはすなわち「金銭的なコストを節約できる」というメリットにも繋がります。大抵のLLMは、AIへの入力とAIからの出力の文字数で課金されるからです。\\n\\n RAGの品質計測\\n\\n「改善は計測から」なので、どのように評価するかは改善前に決めておく必要があります。\\n補足として、こちらの評価指標については、RAG評価ツールの「RAGAS」から、もう少し多くの指標が提案されています。[4]'),\n",
       " Document(metadata={'source': 'https://zenn.dev/knowledgesense/articles/47de9ead8029ba', 'title': 'Zenn'}, page_content='まとめ\\nRAGのテクニックを紹介すると言いながら、この「基礎編」ではいわゆるAdvancedなテクニックは紹介できなかったのが心残りです。（今後の続編にて掲載するためお許しください🙏）\\nただ個人的には、エンジニア向けのチートシートの中にガッツリ、この内容が含まれていることに意味があると思っていて、それは、「ステークホルダーを説得することも、エンジニアが身につけるべきテクニックである」ということだと思うからです。なので、このチートシートを早速社内で展開しています。\\n直近で続編を出す予定で、そこでは「チャンキング」や「ナレッジグラフ」などの手法に触れる予定です。（待てない方は、私の過去の投稿を御覧ください。割と内容か被りそうです...）\\n\\n\\n脚注\\n\\n\\n『A Cheat Sheet and Some Recipes For Building Advanced RAG』 ↩︎\\n\\n\\nRetrieval-Augmented Generation for Large Language Models: A Survey ↩︎'),\n",
       " Document(metadata={'source': 'https://zenn.dev/knowledgesense/articles/47de9ead8029ba', 'title': 'Zenn'}, page_content='最近では、Gemini 1.5など、大量の文章を投下しても精度が落ちづらいモデルも登場してくる流れになっています。（リンク） ↩︎\\n\\n\\nhttps://docs.ragas.io/en/stable/concepts/metrics/index.html ↩︎'),\n",
       " Document(metadata={'source': 'https://zenn.dev/knowledgesense/articles/47de9ead8029ba', 'title': 'Zenn'}, page_content='Atsushi Kadowakiナレッジセンス CEO ← 東大  / エンタープライズ向け生成AIプロダクトで成長中のスタートアップ（2019年~） / ソフトウェアエンジニアを募集中（800万円~）→DM開放中 / 好きな言葉は「実験と学習」/ 最新の生成AI 事情に少し詳しいですナレッジセンス - AI知見共有ブログPublication株式会社ナレッジセンスは、「大企業の知的活動を最速にする」をミッションに掲げ、社内データ検索ができるAIチャットボットを開発・提供しているスタートアップです。このブログでは、LLMや検索技術、RAGの実装戦略などについて知見を共有していきます。DiscussionAtsushi Kadowakiナレッジセンス CEO ← 東大  / エンタープライズ向け生成AIプロダクトで成長中のスタートアップ（2019年~） / ソフトウェアエンジニアを募集中（800万円~）→DM開放中 / 好きな言葉は「実験と学習」/ 最新の生成AI'),\n",
       " Document(metadata={'source': 'https://zenn.dev/knowledgesense/articles/47de9ead8029ba', 'title': 'Zenn'}, page_content='事情に少し詳しいです目次はじめにこの記事は何対象読者本題RAGテクニック集（チートシート）RAGとは。なぜ必要なのか？RAGの仕組みRAGの精度向上のために必要なことRAGを使ってできることRAGの品質計測まとめZennエンジニアのための情報共有コミュニティAboutZennについて運営会社お知らせ・リリースGuides使い方法人向けメニューNewPublication / Proよくある質問LinksX(Twitter)GitHubメディアキットLegal利用規約プライバシーポリシー特商法表記')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits,\n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://zenn.dev/knowledgesense/articles/47de9ead8029ba', 'title': 'Zenn'}, page_content='RAGとは。なぜ必要なのか？\\nRAGとは、大雑把に言うと、ファイルを参照して回答できるLLM（例えばChatGPT）を作成するための方法です。RAGがなぜ必要なのかというと、通常のLLMでは、回答の正確性を向上させるのに限界があるからです。通常のLLMでは、事実と違う内容を勝手に捏造してしまったり（ハルシネーション）、そもそも学習データに含まれていない情報（例えば公開されていない社内の文書）については、回答することはできなかったりという限界があります。\\n\\n\\n RAGの仕組み\\nこの記事をご覧のエンジニアの方であれば、既にご存知の内容かと思います。以下、チートシートの引用です。\\n「RAGでは、ユーザーが質問すると、まず外部データベースから関連するドキュメントを取得する。そのドキュメントと元々のユーザーの質問がセットされ、LLMに渡される。LLMは、この内容をもとに回答を生成する」\\n非常にシンプルな内容なので、この図と一緒に説明すれば、ビジネスサイドの方でも理解してもらえます。私個人的にも、似たような図を使って顧客や社内ビジネスサイドに説明していて、必ず理解してもらえる印象です。'),\n",
       " Document(metadata={'source': 'https://zenn.dev/knowledgesense/articles/47de9ead8029ba', 'title': 'Zenn'}, page_content='RAGの良さはこのシンプルさ、始めやすさなのですが、始めのうちは、なかなか思い通りの回答が得られません。この回答精度を上げようとすると、かなりの苦難が待っています...\\n\\n RAGの精度向上のために必要なこと\\nRAGの精度向上を試みる際に重要な要素は、以下の2点に分解できます。\\n\\nユーザーの質問に回答するために最も必要な（最も関連している）ドキュメント群を抽出すること\\n抽出してきたドキュメント群を最大限上手く活用して、正しい回答を生成すること\\n\\nこの2点は、上の「RAGの仕組み」で登場した画像の中の黄色い枠で囲まれている部分に該当します。\\n※具体的な手法は、今後の記事で紹介していきます\\n\\n RAGを使ってできること\\n\\n個人的には、実務でRAGを使っていて嬉しいポイントは\\n\\n情報不十分なとき、回答しないことができる\\n独自のドキュメントに基づいて回答できる（上の画像には含まれていませんが）'),\n",
       " Document(metadata={'source': 'https://zenn.dev/knowledgesense/articles/47de9ead8029ba', 'title': 'Zenn'}, page_content='事情に少し詳しいです目次はじめにこの記事は何対象読者本題RAGテクニック集（チートシート）RAGとは。なぜ必要なのか？RAGの仕組みRAGの精度向上のために必要なことRAGを使ってできることRAGの品質計測まとめZennエンジニアのための情報共有コミュニティAboutZennについて運営会社お知らせ・リリースGuides使い方法人向けメニューNewPublication / Proよくある質問LinksX(Twitter)GitHubメディアキットLegal利用規約プライバシーポリシー特商法表記'),\n",
       " Document(metadata={'source': 'https://zenn.dev/knowledgesense/articles/47de9ead8029ba', 'title': 'Zenn'}, page_content='Zennナレッジセンス - AI知見共有ブログナレッジセンス - AI知見共有ブログPublicationへの投稿🤖RAGとは？回答精度向上のためのテクニック集（基礎編）Atsushi Kadowaki2024/02/27に公開2024/03/09エンジニアチャットボットChatGPT生成 AIRAGtechはじめまして。株式会社ナレッジセンスの門脇です。普段はエンジニア兼PMとして、「社内データに基づいて回答してくれる」チャットボットをエンタープライズ企業向けに提供しています（一応、200社以上に導入実績あり）。ここで開発しているチャットボットは、ChatGPTを始めとしたLLM（Large Language Models）を活用したサービスであり、その中でもRAG（Retrieval Augmented Generative）という仕組みをガッツリ利用しています。本記事では、RAG精度向上のための知見を共有していきます。\\n\\n\\n はじめに')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"RAGとは？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGとは。なぜ必要なのか？\n",
      "RAGとは、大雑把に言うと、ファイルを参照して回答できるLLM（例えばChatGPT）を作成するための方法です。RAGがなぜ必要なのかというと、通常のLLMでは、回答の正確性を向上させるのに限界があるからです。通常のLLMでは、事実と違う内容を勝手に捏造してしまったり（ハルシネーション）、そもそも学習データに含まれていない情報（例えば公開されていない社内の文書）については、回答することはできなかったりという限界があります。\n",
      "\n",
      "\n",
      " RAGの仕組み\n",
      "この記事をご覧のエンジニアの方であれば、既にご存知の内容かと思います。以下、チートシートの引用です。\n",
      "「RAGでは、ユーザーが質問すると、まず外部データベースから関連するドキュメントを取得する。そのドキュメントと元々のユーザーの質問がセットされ、LLMに渡される。LLMは、この内容をもとに回答を生成する」\n",
      "非常にシンプルな内容なので、この図と一緒に説明すれば、ビジネスサイドの方でも理解してもらえます。私個人的にも、似たような図を使って顧客や社内ビジネスサイドに説明していて、必ず理解してもらえる印象です。\n",
      "\n",
      "RAGの良さはこのシンプルさ、始めやすさなのですが、始めのうちは、なかなか思い通りの回答が得られません。この回答精度を上げようとすると、かなりの苦難が待っています...\n",
      "\n",
      " RAGの精度向上のために必要なこと\n",
      "RAGの精度向上を試みる際に重要な要素は、以下の2点に分解できます。\n",
      "\n",
      "ユーザーの質問に回答するために最も必要な（最も関連している）ドキュメント群を抽出すること\n",
      "抽出してきたドキュメント群を最大限上手く活用して、正しい回答を生成すること\n",
      "\n",
      "この2点は、上の「RAGの仕組み」で登場した画像の中の黄色い枠で囲まれている部分に該当します。\n",
      "※具体的な手法は、今後の記事で紹介していきます\n",
      "\n",
      " RAGを使ってできること\n",
      "\n",
      "個人的には、実務でRAGを使っていて嬉しいポイントは\n",
      "\n",
      "情報不十分なとき、回答しないことができる\n",
      "独自のドキュメントに基づいて回答できる（上の画像には含まれていませんが）\n",
      "\n",
      "事情に少し詳しいです目次はじめにこの記事は何対象読者本題RAGテクニック集（チートシート）RAGとは。なぜ必要なのか？RAGの仕組みRAGの精度向上のために必要なことRAGを使ってできることRAGの品質計測まとめZennエンジニアのための情報共有コミュニティAboutZennについて運営会社お知らせ・リリースGuides使い方法人向けメニューNewPublication / Proよくある質問LinksX(Twitter)GitHubメディアキットLegal利用規約プライバシーポリシー特商法表記\n",
      "\n",
      "Zennナレッジセンス - AI知見共有ブログナレッジセンス - AI知見共有ブログPublicationへの投稿🤖RAGとは？回答精度向上のためのテクニック集（基礎編）Atsushi Kadowaki2024/02/27に公開2024/03/09エンジニアチャットボットChatGPT生成 AIRAGtechはじめまして。株式会社ナレッジセンスの門脇です。普段はエンジニア兼PMとして、「社内データに基づいて回答してくれる」チャットボットをエンタープライズ企業向けに提供しています（一応、200社以上に導入実績あり）。ここで開発しているチャットボットは、ChatGPTを始めとしたLLM（Large Language Models）を活用したサービスであり、その中でもRAG（Retrieval Augmented Generative）という仕組みをガッツリ利用しています。本記事では、RAG精度向上のための知見を共有していきます。\n",
      "\n",
      "\n",
      " はじめに\n"
     ]
    }
   ],
   "source": [
    "print(format_docs(retriever.invoke(\"RAGとは？\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG（Retrieval Augmented Generative）とは、ファイルを参照して回答できる大規模言語モデル（LLM）を作成するための方法です。RAGは、外部データベースから関連するドキュメントを取得し、それを基に回答を生成することで、通常のLLMの限界を克服します。これにより、より正確な情報を提供できるようになります。\n"
     ]
    }
   ],
   "source": [
    "print(rag_chain.invoke(\"RAGとはなんですか？\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGのメリットは、シンプルさと始めやすさにあります。また、情報が不十分な場合には回答を控えることができ、独自のドキュメントに基づいて正確な回答を生成することが可能です。これにより、回答の精度を向上させることが期待できます。\n"
     ]
    }
   ],
   "source": [
    "print(rag_chain.invoke(\"RAGのメリットは？\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part2: Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"私のお気に入りのペットの種類はなんですか？\"\n",
    "document = \"私のお気に入りのペットは猫です。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "num_tokens_from_string(question, \"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embed = OpenAIEmbeddings()\n",
    "query_result = embed.embed_query(question)\n",
    "document_result = embed.embed_query(document)\n",
    "len(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.9325834309095222\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "similarity = cosine_similarity(query_result, document_result)\n",
    "print(\"Cosine Similarity:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://zenn.dev/knowledgesense/articles/47de9ead8029ba\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"Container_wide__ykGLh Container_common__figYY\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(blog_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part3: Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://zenn.dev/knowledgesense/articles/47de9ead8029ba', 'title': 'Zenn'}, page_content='RAGの仕組み\\nこの記事をご覧のエンジニアの方であれば、既にご存知の内容かと思います。以下、チートシートの引用です。\\n「RAGでは、ユーザーが質問すると、まず外部データベースから関連するドキュメントを取得する。そのドキュメントと元々のユーザーの質問がセットされ、LLMに渡される。LLMは、この内容をもとに回答を生成する」\\n非常にシンプルな内容なので、この図と一緒に説明すれば、ビジネスサイドの方でも理解してもらえます。私個人的にも、似たような図を使って顧客や社内ビジネスサイドに説明していて、必ず理解してもらえる印象です。\\n\\nRAGの良さはこのシンプルさ、始めやすさなのですが、始めのうちは、なかなか思い通りの回答が得られません。この回答精度を上げようとすると、かなりの苦難が待っています...')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(\"RAGとはなんですか？\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part4: Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='RAGは、ユーザーが質問すると外部データベースから関連するドキュメントを取得し、LLMに渡して回答を生成する仕組みのことです。', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 58, 'prompt_tokens': 397, 'total_tokens': 455, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3850b5c6-f579-4db9-83f1-c872547c0325-0', usage_metadata={'input_tokens': 397, 'output_tokens': 58, 'total_tokens': 455, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "chain = prompt | llm\n",
    "chain.invoke({\"context\": docs, \"question\": \"RAGとはなんですか？\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt_hub_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "prompt_hub_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAGは、ユーザーが質問すると外部データベースから関連するドキュメントを取得し、LLMに渡して回答を生成する仕組みのことです。'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"RAGとはなんですか？\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning-langchain-BRoTc1ZH-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
