{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-back Prompting\n",
    "\n",
    "1. ユーザーの元の質問に基づいて、ステップバック質問を生成\n",
    "2. 元の質問とステップバック質問の両方を情報収集\n",
    "3. 取得した両方の情報に基づいて回答を生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# Load blog\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://zenn.dev/knowledgesense/articles/47de9ead8029ba\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"Container_wide__ykGLh Container_common__figYY\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()\n",
    "\n",
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500, \n",
    "    chunk_overlap=50)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "# Index\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotChatMessagePromptTemplate(examples=[{'input': '警察官は合法的な逮捕を行うことができますか？', 'output': '警察官は何ができるのですか？'}, {'input': 'Jan Sindelはどこの国で生まれましたか？', 'output': 'Jan Sindelの個人史は何ですか？'}], input_variables=[], input_types={}, partial_variables={}, example_prompt=ChatPromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], input_types={}, partial_variables={}, template='{output}'), additional_kwargs={})]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"警察官は合法的な逮捕を行うことができますか？\",\n",
    "        \"output\": \"警察官は何ができるのですか？\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindelはどこの国で生まれましたか？\",\n",
    "        \"output\": \"Jan Sindelの個人史は何ですか？\",\n",
    "    },\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "few_shot_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='あなたは世界の知識のエキスパートです。あなたの仕事は、一歩下がって、より答えやすい一般的な一歩下がった質問に言い換えることです。いくつか例を挙げましょう：'), additional_kwargs={}), FewShotChatMessagePromptTemplate(examples=[{'input': '警察官は合法的な逮捕を行うことができますか？', 'output': '警察官は何ができるのですか？'}, {'input': 'Jan Sindelはどこの国で生まれましたか？', 'output': 'Jan Sindelの個人史は何ですか？'}], input_variables=[], input_types={}, partial_variables={}, example_prompt=ChatPromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], input_types={}, partial_variables={}, template='{output}'), additional_kwargs={})])), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"あなたは世界の知識のエキスパートです。あなたの仕事は、一歩下がって、より答えやすい一般的な一歩下がった質問に言い換えることです。いくつか例を挙げましょう：\"),\n",
    "        few_shot_prompt,\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'タスク分解とは何ですか？'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "generate_queries_step_back = prompt | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0) | StrOutputParser()\n",
    "question = \"LLMエージェントにおけるタスク分解とは何ですか？\"\n",
    "generate_queries_step_back.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG（Retrieval-Augmented Generation）とは、ファイルを参照して回答できる大規模言語モデル（LLM）を作成するための手法です。RAGの主な目的は、通常のLLMが持つ限界を克服することにあります。具体的には、通常のLLMは事実と異なる情報を生成する「ハルシネーション」や、学習データに含まれていない情報に対して正確な回答ができないという問題があります。\n",
      "\n",
      "RAGの仕組みはシンプルで、ユーザーが質問をすると、まず外部データベースから関連するドキュメントを取得し、そのドキュメントとユーザーの質問をセットにしてLLMに渡します。LLMはこの情報を基に回答を生成します。このプロセスにより、より正確で関連性の高い回答を得ることが可能になります。\n",
      "\n",
      "RAGを利用することで、情報が不十分な場合には回答をしない選択ができたり、独自のドキュメントに基づいて回答することができるため、実務においても非常に有用です。\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "response_prompt_template = \"\"\"あなたは世界を知るエキスパートです。\n",
    "私はあなたに質問します。\n",
    "あなたの回答は包括的であるべきであり、以下の文脈が関連するのであれば、\n",
    "それらと矛盾しないものでなければならない。\n",
    "そうでなければ、関連性がなければ無視してください。\n",
    "\n",
    "# {normal_context}\n",
    "# {step_back_context}\n",
    "\n",
    "# オリジナルの質問: {question}\n",
    "# 回答:\"\"\"\n",
    "\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        # invokeで文字列を渡すのではなく {\"question\": \"質問\"} というdictで渡しているため整形が必要\n",
    "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
    "        \"step_back_context\": generate_queries_step_back | retriever,\n",
    "        \"question\": lambda x: x[\"question\"]\n",
    "    }\n",
    "    | response_prompt\n",
    "    | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "question = \"RAGとはなんですか？\"\n",
    "print(chain.invoke({\"question\": question}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning-langchain-BRoTc1ZH-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
