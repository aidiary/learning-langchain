{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposition（クエリ分解）\n",
    "\n",
    "- 与えられた質問を複数のサブクエリに分解\n",
    "- サブクエリの回答を組み合わせて元の質問の回答を得る\n",
    "- サブクエリを順番に処理して、次のサブクエリに前のサブクエリの結果を渡すケース（Perplexity.AI）\n",
    "- サブクエリを並列に処理するケースがある（Genspark）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# Load blog\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://zenn.dev/knowledgesense/articles/47de9ead8029ba\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"Container_wide__ykGLh Container_common__figYY\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()\n",
    "\n",
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500, \n",
    "    chunk_overlap=50)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "# Index\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"あなたは、入力された質問に関連する複数のサブ質問を生成する役に立つアシスタントです。\n",
    "目的は、入力された質問を、個別に回答できる一連のサブ問題やサブ質問に分解することです。\n",
    "次の質問に関連する複数の検索クエリを生成してください。\n",
    "\n",
    "質問: {question}\n",
    "\n",
    "出力（3つのクエリ）:\"\"\"\n",
    "\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "generate_queries_decomposition = (\n",
    "    prompt_decomposition\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. RAGの略は何ですか？', '2. RAGはどのような分野で使用される言葉ですか？', '3. RAGの歴史や起源について何か情報がありますか？']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_queries_decomposition.invoke({\"question\": \"RAGとはなんですか？\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. アカウント削除手順', '2. アカウント削除の注意事項', '3. アカウント削除後のデータの取り扱い']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_queries_decomposition.invoke({\"question\": \"アカウントを削除したいです\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. ログイン画面に表示されるエラーメッセージは何ですか？',\n",
       " '2. 最後にパスワードを変更した日付はわかりますか？',\n",
       " '3. ログインできなくなったアカウントは、他のデバイスからもアクセスできない状態ですか？']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_queries_decomposition.invoke({\"question\": \"ログインできなくなりました\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"こちらがあなたが回答すべき質問です:\n",
    "{question}\n",
    "\n",
    "こちらは利用可能な背景質問+回答のペアです:\n",
    "{q_a_pairs}\n",
    "\n",
    "こちらは質問に関連する追加のコンテキストです:\n",
    "{context}\n",
    "\n",
    "上記のコンテキストと背景の質問+回答ペアを使用して、次の質問に回答してください:\n",
    "{question}\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. RAGの略称は何ですか？', '2. RAGはどのような分野で使用される言葉ですか？', '3. RAGの歴史や起源について知りたいです。']\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"質問: {question}\\n回答: {answer}\\n\\n\"\n",
    "    return formatted_string\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "questions = generate_queries_decomposition.invoke({\"question\": \"RAGとはなんですか？\"})\n",
    "print(questions)\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "\n",
    "# 1つずつ順番に質問に回答\n",
    "for q in questions:\n",
    "    rag_chain = (\n",
    "        {\"context\": itemgetter(\"question\") | retriever,\n",
    "         \"question\": itemgetter(\"question\"),\n",
    "         \"q_a_pairs\": itemgetter(\"q_a_pairs\")}\n",
    "        | decomposition_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    answer = rag_chain.invoke({\"question\": q, \"q_a_pairs\": q_a_pairs})\n",
    "\n",
    "    # 前のクエリの質問と回答のペアを追加\n",
    "    q_a_pair = format_qa_pair(q, answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n\" + q_a_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG（Retrieval-Augmented Generation）の歴史や起源については、具体的な年や発表に関する詳細な情報は限られていますが、RAGの概念は、自然言語処理（NLP）や人工知能（AI）の進化の中で生まれました。\n",
      "\n",
      "RAGは、従来の大規模言語モデル（LLM）の限界を克服するために開発された手法の一つです。従来のLLMは、学習データに基づいて生成された回答を提供するため、事実と異なる情報を生成する「ハルシネーション」や、学習データに含まれていない情報に対する回答ができないという問題がありました。これに対処するために、外部データベースから関連するドキュメントを取得し、それを基にして回答を生成するRAGのアプローチが提案されました。\n",
      "\n",
      "RAGの登場により、情報検索や質問応答システムにおいて、より正確で信頼性の高い情報提供が可能になりました。この手法は、特にビジネスや教育、カスタマーサポートなどの実務の場面での応用が進んでいます。\n",
      "\n",
      "RAGの具体的な技術的な発展や研究は、近年の生成AIの進化とともに進んでおり、今後もさらなる研究や実装が期待されています。\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# クエリを並列に検索する場合\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def retrieve_and_rag(question, prompt_rag, sub_question_generator_chain):\n",
    "    # decompositionでクエリを分解\n",
    "    sub_questions = sub_question_generator_chain.invoke({\"question\": question})\n",
    "\n",
    "    rag_results = []\n",
    "    for sub_question in sub_questions:\n",
    "        retrieved_docs = retriever.get_relevant_documents(sub_question)\n",
    "        answer = (prompt_rag | llm | StrOutputParser()).invoke({\"context\": retrieved_docs, \"question\": sub_question})\n",
    "        rag_results.append(answer)\n",
    "\n",
    "    return rag_results, sub_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"RAGとはなんですか？\"\n",
    "\n",
    "answers, questions = retrieve_and_rag(question, prompt_rag, generate_queries_decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RAGの略は「Retrieval-Augmented Generation」です。これは、外部データベースから関連するドキュメントを取得し、それを基に回答を生成する方法です。RAGは、通常のLLMの限界を克服するために必要とされています。',\n",
       " 'RAGは、主に情報検索や自然言語処理の分野で使用される言葉です。特に、ユーザーの質問に対して関連するドキュメントを参照し、正確な回答を生成するための手法として重要です。これにより、従来の言語モデルの限界を克服することが可能になります。',\n",
       " 'RAG（Retrieval-Augmented Generation）は、外部データベースから関連するドキュメントを取得し、それを基に回答を生成するLLMの手法です。この技術は、通常のLLMの限界を克服し、正確な情報を提供するために必要とされています。具体的な歴史や起源についての詳細は、提供された文脈には含まれていません。']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. RAGの略は何ですか？', '2. RAGはどのような分野で使用される言葉ですか？', '3. RAGの歴史や起源について知りたいです。']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "質問 1: 1. RAGの略は何ですか？\n",
      "回答 1: RAGの略は「Retrieval-Augmented Generation」です。これは、外部データベースから関連するドキュメントを取得し、それを基に回答を生成する方法です。RAGは、通常のLLMの限界を克服するために必要とされています。\n",
      "\n",
      "質問 2: 2. RAGはどのような分野で使用される言葉ですか？\n",
      "回答 2: RAGは、主に情報検索や自然言語処理の分野で使用される言葉です。特に、ユーザーの質問に対して関連するドキュメントを参照し、正確な回答を生成するための手法として重要です。これにより、従来の言語モデルの限界を克服することが可能になります。\n",
      "\n",
      "質問 3: 3. RAGの歴史や起源について知りたいです。\n",
      "回答 3: RAG（Retrieval-Augmented Generation）は、外部データベースから関連するドキュメントを取得し、それを基に回答を生成するLLMの手法です。この技術は、通常のLLMの限界を克服し、正確な情報を提供するために必要とされています。具体的な歴史や起源についての詳細は、提供された文脈には含まれていません。\n"
     ]
    }
   ],
   "source": [
    "def format_qa_pairs(questions, answers):\n",
    "    \"\"\"Format Q and A pairs\"\"\"\n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"質問 {i}: {question}\\n回答 {i}: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "\n",
    "context = format_qa_pairs(questions, answers)\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG（Retrieval-Augmented Generation）とは、外部データベースから関連するドキュメントを取得し、それを基に回答を生成する手法です。この技術は、情報検索や自然言語処理の分野で主に使用されており、ユーザーの質問に対して正確な回答を提供するために重要です。RAGは、従来の言語モデルの限界を克服するために開発され、より正確な情報を提供することを目的としています。\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"ここに質問と回答のペアがあります:\n",
    "\n",
    "{context}\n",
    "\n",
    "これらを使って、次の質問に対する答えをまとめてください: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(final_rag_chain.invoke({\"context\": context, \"question\": question}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning-langchain-BRoTc1ZH-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
