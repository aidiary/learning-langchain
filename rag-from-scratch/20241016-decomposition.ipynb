{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposition（クエリ分解）\n",
    "\n",
    "- 与えられた質問を複数のサブクエリに分解\n",
    "- サブクエリの回答を組み合わせて元の質問の回答を得る\n",
    "- サブクエリを順番に処理して、次のサブクエリに前のサブクエリの結果を渡すケース（Perplexity.AI）\n",
    "- サブクエリを並列に処理するケースがある（Genspark）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# Load blog\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://zenn.dev/knowledgesense/articles/47de9ead8029ba\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"Container_wide__ykGLh Container_common__figYY\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()\n",
    "\n",
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500, \n",
    "    chunk_overlap=50)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "# Index\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"あなたは、入力された質問に関連する複数のサブ質問を生成する役に立つアシスタントです。\n",
    "目的は、入力された質問を、個別に回答できる一連のサブ問題やサブ質問に分解することです。\n",
    "次の質問に関連する複数の検索クエリを生成してください。\n",
    "\n",
    "質問: {question}\n",
    "\n",
    "出力（3つのクエリ）:\"\"\"\n",
    "\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "generate_queries_decomposition = (\n",
    "    prompt_decomposition\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. RAGの略は何ですか？', '2. RAGはどのような分野で使用される言葉ですか？', '3. RAGの歴史や起源について何か情報がありますか？']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_queries_decomposition.invoke({\"question\": \"RAGとはなんですか？\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. アカウント削除手順', '2. アカウント削除の注意事項', '3. アカウント削除後のデータの取り扱い']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_queries_decomposition.invoke({\"question\": \"アカウントを削除したいです\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. ログイン画面に表示されるエラーメッセージは何ですか？',\n",
       " '2. 最後にパスワードを変更した日付はわかりますか？',\n",
       " '3. ログインできなくなったアカウントは、他のデバイスからもアクセスできない状態ですか？']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_queries_decomposition.invoke({\"question\": \"ログインできなくなりました\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"こちらがあなたが回答すべき質問です:\n",
    "{question}\n",
    "\n",
    "こちらは利用可能な背景質問+回答のペアです:\n",
    "{q_a_pairs}\n",
    "\n",
    "こちらは質問に関連する追加のコンテキストです:\n",
    "{context}\n",
    "\n",
    "上記のコンテキストと背景の質問+回答ペアを使用して、次の質問に回答してください:\n",
    "{question}\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. RAGの略称は何ですか？', '2. RAGはどのような分野で使用される言葉ですか？', '3. RAGの歴史や起源について知りたいです。']\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"質問: {question}\\n回答: {answer}\\n\\n\"\n",
    "    return formatted_string\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "questions = generate_queries_decomposition.invoke({\"question\": \"RAGとはなんですか？\"})\n",
    "print(questions)\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "\n",
    "# 1つずつ順番に質問に回答\n",
    "for q in questions:\n",
    "    rag_chain = (\n",
    "        {\"context\": itemgetter(\"question\") | retriever,\n",
    "         \"question\": itemgetter(\"question\"),\n",
    "         \"q_a_pairs\": itemgetter(\"q_a_pairs\")}\n",
    "        | decomposition_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    answer = rag_chain.invoke({\"question\": q, \"q_a_pairs\": q_a_pairs})\n",
    "\n",
    "    # 前のクエリの質問と回答のペアを追加\n",
    "    q_a_pair = format_qa_pair(q, answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n\" + q_a_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG（Retrieval-Augmented Generation）の歴史や起源については、具体的な年や発表に関する詳細な情報は限られていますが、RAGの概念は、自然言語処理（NLP）や人工知能（AI）の進化の中で生まれました。\n",
      "\n",
      "RAGは、従来の大規模言語モデル（LLM）の限界を克服するために開発された手法の一つです。従来のLLMは、学習データに基づいて生成された回答を提供するため、事実と異なる情報を生成する「ハルシネーション」や、学習データに含まれていない情報に対する回答ができないという問題がありました。これに対処するために、外部データベースから関連するドキュメントを取得し、それを基にして回答を生成するRAGのアプローチが提案されました。\n",
      "\n",
      "RAGの登場により、情報検索や質問応答システムにおいて、より正確で信頼性の高い情報提供が可能になりました。この手法は、特にビジネスや教育、カスタマーサポートなどの実務の場面での応用が進んでいます。\n",
      "\n",
      "RAGの具体的な技術的な発展や研究は、近年の生成AIの進化とともに進んでおり、今後もさらなる研究や実装が期待されています。\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# クエリを並列に検索する場合\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning-langchain-BRoTc1ZH-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
